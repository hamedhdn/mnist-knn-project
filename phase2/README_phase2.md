# Phase 2 â€“ Manual KNN Implementation on MNIST ğŸ§ 

In this phase of the project, the goal is to manually implement the **K-Nearest Neighbors (KNN)** algorithm using NumPy and compare its performance with the built-in version from scikit-learn on the MNIST digit classification dataset.

---

## ğŸ¯ Objectives
- Gain a deeper understanding of how KNN works internally  
- Build a custom KNN classifier from scratch using NumPy  
- Compare accuracy and performance with scikit-learn's optimized KNN  

---

## ğŸ“ Project Structure
```
phase2/
â”œâ”€â”€ knn_numpy.py       # Manual implementation of KNN (knn_predict function)
â”œâ”€â”€ compare_knn.py     # Accuracy comparison between manual and sklearn
â””â”€â”€ README.md
```

---

## ğŸ“¦ Required Libraries
Install the following dependencies before running the scripts:
```bash
pip install numpy scikit-learn keras
```

---

## ğŸ›  Steps

1. **Load MNIST Dataset**  
   - Using `keras.datasets`, load the 28Ã—28 grayscale digit images and flatten them.  

2. **Normalize the Data**  
   - Scale pixel values to the range `[0, 1]` by dividing by 255.  

3. **Implement `knn_predict` Function**  
   - Compute Euclidean distances  
   - Identify *k* nearest neighbors  
   - Use majority voting to assign labels  

4. **Compare with scikit-learn's KNN**  
   - Use `KNeighborsClassifier` to benchmark accuracy and speed.  

5. **Evaluate Accuracy**  
   - Accuracy is calculated using `accuracy_score` from `sklearn.metrics`.  

---

## ğŸ“Š Sample Results
```
Accuracy (Manual KNN):       0.9900
Accuracy (Scikit-learn KNN): 0.9900
```

---

## ğŸš€ Suggestions for Improvement
- Use **vectorized operations** to speed up the manual implementation  
- Test on **smaller subsets** to reduce runtime  
- Explore **fast nearest neighbor libraries** like [FAISS](https://github.com/facebookresearch/faiss) or [HNSWlib](https://github.com/nmslib/hnswlib) for large-scale applications  

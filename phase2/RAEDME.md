# Phase 2 â€“ Manual KNN Implementation on MNIST ğŸ§ 

In this phase of the project, the goal is to manually implement the K-Nearest Neighbors (KNN) algorithm using NumPy and compare its performance with the built-in version from scikit-learn on the MNIST digit classification dataset.

## ğŸ¯ Objectives
- Gain a deeper understanding of how KNN works internally
- Build a custom KNN classifier from scratch using NumPy
- Compare accuracy and performance with scikit-learn's optimized KNN

## ğŸ“ Project Structure

phase2/
â”œâ”€â”€ knn_numpy.py # Manual implementation of knn_predict 
â”œâ”€â”€ compare_knn.py # Accuracy comparison between manual and sklearn 
â”œâ”€â”€ README.md


## ğŸ“¦ Required Libraries
```bash
pip install numpy scikit-learn keras



## ğŸ› ï¸ Steps

1.Load MNIST Dataset
Using keras.datasets, the 28Ã—28 grayscale digit images are loaded and flattened.

2.Normalize the Data
Pixel values are scaled to the range [0, 1] by dividing by 255.

3.Implement knn_predict Function

   .Compute Euclidean distances

   .Identify k nearest neighbors

   .Use majority voting to assign labels

4.Compare with scikit-learn's KNN 
Use KNeighborsClassifier to benchmark accuracy and speed.

5.Evaluate Accuracy 
Accuracy is calculated using accuracy_score from sklearn.metrics.



## ğŸ“Š Sample Results

Accuracy (Manual KNN): 0.9900
Accuracy (Scikit-learn KNN): 0.9900


## ğŸš€ Suggestions for Improvement

    .Use vectorized operations to speed up the manual implementation

    .Test on smaller subsets to reduce runtime

    .Explore fast nearest neighbor libraries like FAISS or HNSWlib for large-scale applications